{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import os, glob, json, xarray, nibabel\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nilearn.input_data import NiftiLabelsMasker\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from nilearn.plotting import plot_roi\n",
    "from nilearn.image import mean_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set main directories.\n",
    "aomic_dir = '/mnt/mbServerData/data/AOMIC/'\n",
    "main_dir = '/raid/projects/Emin/AOMIC/'\n",
    "output_dir = os.path.join(main_dir, 'preprocessing')\n",
    "\n",
    "# Define aomic_datasets\n",
    "aomic_datasets = ['ID1000']\n",
    "\n",
    "# Define tasks\n",
    "tasks = ['moviewatching']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some parameters\n",
    "fwhm = 6\n",
    "high_pass = 1/128\n",
    "n_cores = 30\n",
    "save_figs = True\n",
    "\n",
    "# Atlas to parcellate mri images.\n",
    "atlas_dir = os.path.join(main_dir, 'atlasses', 'shen_1mm_268_parcellation.nii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define important functions\n",
    "def organize_confound_mat(confounds):\n",
    "    \"\"\"Constructs a confound mat comprising tCompCor compounds and\n",
    "    24 motion parameters.\"\"\"\n",
    "    t_comp_cors = [c for c in confounds.columns if 't_comp_cor' in c]\n",
    "    t_comp_cors = confounds.loc[:, t_comp_cors[0]:t_comp_cors[-1]]\n",
    "    mot_params = confounds.loc[:, 'trans_x':'rot_z_power2']\n",
    "    return pd.concat([mot_params, t_comp_cors], axis=1).fillna(0)\n",
    "\n",
    "def extract_time_series(task, subdir, aomic_dir, aomic_dataset, atlas_dir, \n",
    "    tr=None, save_figs=True, output_figure_dir=None, verbose=True):\n",
    "    \"\"\"Extracts time series from brain images using a given parcellation atlas.\n",
    "    While extracting the time series, it also performs confound correction \n",
    "    (e.g., removing tCompCors and 24 motion parameters), high-pass filtering and smoothing.\"\"\"\n",
    "    sub_name = subdir[-8:]\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Subject: {sub_name}\\r', end='', flush=True)\n",
    "\n",
    "    task_data_list = glob.glob(os.path.join(subdir, 'func', f'{sub_name}_task-{task}*'))\n",
    "\n",
    "    if task_data_list:\n",
    "        # task_sub_list.append(sub_name)\n",
    "        for task_data_dir in task_data_list:\n",
    "            if 'MNI152NLin2009cAsym_desc-preproc_bold.nii.gz' in task_data_dir:\n",
    "                # find fmri image dir.\n",
    "                mri_dir = task_data_dir\n",
    "            if 'confounds_regressors.tsv' in task_data_dir:\n",
    "                # Find confound dir.\n",
    "                confound_dir = task_data_dir\n",
    "\n",
    "        # Extract TR from imaging json file.\n",
    "        if tr is None:\n",
    "            img_info_dir = glob.glob(\n",
    "                os.path.join(\n",
    "                    aomic_dir, aomic_dataset, sub_name, 'func', f'{sub_name}_task-{task}*_bold.json'))[0]\n",
    "            with open(img_info_dir) as f:\n",
    "                tr = json.load(f)['RepetitionTime']\n",
    "\n",
    "        # Save figure showing MRI images overlayed with atlas if desired.\n",
    "        if save_figs:\n",
    "            assert output_figure_dir is not None, 'You need to enter output directory for figures!'\n",
    "            img = nibabel.load(mri_dir)\n",
    "            plot_roi(\n",
    "                atlas_dir, mean_img(img), title=sub_name, \n",
    "                output_file=os.path.join(output_figure_dir, f'{sub_name}.png'))\n",
    "\n",
    "        # Prepare confound mat and extract time series.\n",
    "        confound_mat = organize_confound_mat(pd.read_csv(confound_dir, delimiter='\\t'))\n",
    "        time_series = NiftiLabelsMasker(\n",
    "            atlas_dir, high_pass=high_pass, t_r=tr, smoothing_fwhm=fwhm, standardize=True).fit_transform(mri_dir, confound_mat)\n",
    "        # task_time_series.append(time_series)\n",
    "        return sub_name, time_series\n",
    "\n",
    "def construct_connectivity_matrices(time_series, corr_type='correlation', vectorize=False):\n",
    "    \"\"\"Constructs connectivity matrices using given time series.\"\"\"\n",
    "    conn = ConnectivityMeasure(kind = corr_type, vectorize=vectorize, discard_diagonal=True)\n",
    "    return conn.fit_transform(np.array(time_series))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract time series from the MRI images using given atlas.\n",
    "# Smoothing, high-pass filtering and confound regression are also applied.\n",
    "tr = 2.2\n",
    "for aomic_dataset in aomic_datasets:\n",
    "    print(f'Dataset: {aomic_dataset}')\n",
    "    sub_list = [f for f in glob.glob(os.path.join(aomic_dir, aomic_dataset,'derivatives','fmriprep','sub-*')) if os.path.isdir(f)]\n",
    "\n",
    "    task_data_dict = {}\n",
    "    for task in tasks:    \n",
    "        print(f'\\nTask: {task}')\n",
    "        task_time_series = []\n",
    "        task_sub_list = []      \n",
    "\n",
    "        if save_figs: \n",
    "            output_figure_dir = os.path.join(output_dir, aomic_dataset, 'figures', task,'')\n",
    "            if not os.path.exists(output_figure_dir):\n",
    "                os.makedirs(output_figure_dir)\n",
    "            \n",
    "        results = Parallel(n_jobs=n_cores)(delayed(extract_time_series)(\n",
    "            task, subdir, aomic_dir, aomic_dataset, atlas_dir, tr, True, output_figure_dir) for subdir in tqdm(sub_list))\n",
    "        results = list(zip(*list(filter(None, results))))\n",
    "        task_data_dict[task] = xarray.Dataset(\n",
    "            {'time_series': xarray.DataArray(list(results[1]), dims=('ID', 'timeseries', 'nodes')), 'ID': list(results[0])})\n",
    "\n",
    "    # Save time series into a netCDF file.\n",
    "    for t in task_data_dict:\n",
    "        task_data_dict[t].to_netcdf(os.path.join(output_dir, aomic_dataset, f'time_series_{t}.nc'))\n",
    "\n",
    "print('\\nDone')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct connectivity matrices using the timeseries.\n",
    "# Pearson-correlation coefficient.\n",
    "for aomic_dataset in aomic_datasets:\n",
    "    print(f'Dataset: {aomic_dataset}')\n",
    "    for task in tasks: \n",
    "        print(f'Task: {task}')\n",
    "        # Define directory for time series.\n",
    "        time_series_dir = os.path.join(output_dir, aomic_dataset, f'time_series_{task}.nc')\n",
    "        time_series = xarray.open_dataset(time_series_dir) # Load time series data.\n",
    "\n",
    "        # Compute vectorized connectivity matrices and save it into a .csv file.\n",
    "        conn_mat_vectorized = construct_connectivity_matrices(time_series.time_series, vectorize=True)\n",
    "        pd.concat(\n",
    "            [pd.Series(time_series.ID), pd.DataFrame(conn_mat_vectorized)], axis=1).to_csv(\n",
    "                os.path.join(output_dir, aomic_dataset, f'conn_mat_vectorized_{task}.csv'), index=None\n",
    "            )\n",
    "\n",
    "        # Compute connectivity matrices (symmetric matrix) and save it into a .nc file.\n",
    "        time_series['conn_mat'] = (('ID', 'nodes', 'nodes'), \n",
    "            construct_connectivity_matrices(time_series.time_series))\n",
    "        time_series.drop('time_series').to_netcdf(\n",
    "            os.path.join(output_dir, aomic_dataset, f'conn_mat_{task}.nc'))\n",
    "        \n",
    "print('Done!!!')       \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "631ded74277cc0aadea4115b1ef9d8d6321c4badbb2317ea1c00f7f553b8eb93"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
