{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import os, xarray\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set main directories.\n",
    "main_dir = '/raid/projects/Emin/AOMIC/'\n",
    "data_dir = os.path.join(main_dir, 'preprocessing')\n",
    "\n",
    "# Atlases\n",
    "atlases = ['Shen_268']\n",
    "\n",
    "# Task and dataset\n",
    "task = 'moviewatching'\n",
    "dataset = 'ID1000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_conn_mat(conn_mat, output_dir, sub_name):\n",
    "    \"\"\"Saves connectivity matrices into single csv files.\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    conn_mat.to_csv(os.path.join(output_dir, f'{sub_name}.csv'), header=None, index=None)\n",
    "\n",
    "def remove_nans(data):\n",
    "    \"\"\"Remove nans from given data.\"\"\"\n",
    "    data = pd.DataFrame(data)\n",
    "    nan_mask = np.ravel(np.array(data.isna()))\n",
    "    data = data[~nan_mask]\n",
    "    return data, nan_mask\n",
    "\n",
    "def replace_outlier(data, cutoff=1.5):\n",
    "    \"\"\"Replace outliers with nan.\n",
    "    Outliers are detected as being lower or upper\n",
    "    than given cutoff value (e.g., 1.5) times \n",
    "    interquantile range\"\"\"\n",
    "    data = pd.Series(data)\n",
    "    q25, q75 = data.quantile(0.25), data.quantile(0.75)\n",
    "    iqr = q75 - q25\n",
    "    lower, upper = q25 - iqr*cutoff, q75 + iqr*cutoff\n",
    "    outlier_map = (data < lower) | (data > upper)\n",
    "    data[outlier_map] = np.nan\n",
    "    return data, outlier_map\n",
    "\n",
    "def organize_save_dataset(main_dir, atlas, dataset, task, remove_outlier=False, target=None, cutoff=1.5, fd=0.5):\n",
    "    \"\"\"Organizes dataset.\n",
    "    \n",
    "    It removes subjects with excessive head motion computed using frame-wise displacement,\n",
    "    and with outlier target values (e.g., IQR*1.5)\n",
    "    \"\"\"\n",
    "    # Define output directory.\n",
    "    output_dir = os.path.join(main_dir, 'data_organization', atlas, dataset)\n",
    "\n",
    "    # Load connectome dataset.\n",
    "    data = xarray.load_dataarray(os.path.join(main_dir, 'preprocessing', atlas, dataset, f'conn_mat_{task}.nc'))\n",
    "    ID_array = np.array(data.ID)\n",
    "\n",
    "    # Load behavioral data.\n",
    "    beh_data = pd.read_csv(\n",
    "        os.path.join(main_dir, 'data', f'{dataset}_participants.csv')).rename({'participant_id':'ID'}, axis=1)\n",
    "    beh_data.sex = beh_data.sex.replace({'female':1, 'male':0, 'F':1, 'M':0})\n",
    "\n",
    "    # Remove nans.\n",
    "    _, nan_mask = remove_nans(beh_data[target])\n",
    "    sub_to_remove = np.array(beh_data.ID.loc[nan_mask])\n",
    "\n",
    "    # Remove outlier if desired.\n",
    "    if remove_outlier:\n",
    "        assert target is not None, 'Target variable must be provided!'\n",
    "        beh_data[target], outlier_map = replace_outlier(beh_data[target], cutoff=cutoff)\n",
    "        sub_to_remove = np.concatenate([sub_to_remove, np.array(beh_data.ID.loc[outlier_map])])\n",
    "        beh_data = beh_data[~outlier_map].reset_index(drop=True)\n",
    "\n",
    "    # Load subject motion\n",
    "    sub_motion = pd.read_csv(os.path.join(main_dir, 'data', 'subject_motion.csv'))\n",
    "    sub_high_motion = np.array(\n",
    "        sub_motion.ID[(sub_motion.dataset == dataset) & (sub_motion.task == task) & (sub_motion.FD > fd)])\n",
    "    \n",
    "    # Load total intracranial volume\n",
    "    sub_TIV = pd.read_csv(os.path.join(main_dir, 'data', 'subject_TIV.csv'))\n",
    "    sub_TIV = sub_TIV[(sub_TIV.dataset == dataset)]\n",
    "\n",
    "    # Subjects to remove.\n",
    "    sub_to_remove = np.unique(np.concatenate([sub_to_remove, sub_high_motion]))\n",
    "\n",
    "    # Save conn matrices into .csv files\n",
    "    clean_ID_array = []\n",
    "    for ID in ID_array:\n",
    "        if ID not in sub_to_remove:\n",
    "            save_conn_mat(\n",
    "                pd.DataFrame(np.array(data.loc[ID])), \n",
    "                os.path.join(output_dir, 'conn_mat'), ID)\n",
    "            clean_ID_array.append(ID)\n",
    "\n",
    "    # Remove beh data that has no fMRI images, and save into a .csv file.\n",
    "    beh_data = beh_data.merge(\n",
    "        pd.DataFrame(clean_ID_array).rename({0:'ID'}, axis=1), on='ID', how='right')\n",
    "    beh_data = beh_data.merge(sub_TIV, on='ID', how='left')\n",
    "    beh_data.to_csv(os.path.join(output_dir, 'behavioral_data.csv'), index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the final dataset. \n",
    "# Removes subjects with outlier target values using very liberal threshold of IQR*2.5\n",
    "# Removes subjects with excessive head motion (FD > 0.5)\n",
    "organize_save_dataset(main_dir, atlas='Shen_268', dataset=dataset, task=task, \n",
    "                    remove_outlier=True, target='IST_fluid', \n",
    "                    cutoff=2.5, fd=0.5)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "374146b879f64c40364024d3c7a67bd9057a636d195bdcec9dcae3eaf2e8c489"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit (conda)",
   "language": "python",
   "name": "python394jvsc74a57bd0374146b879f64c40364024d3c7a67bd9057a636d195bdcec9dcae3eaf2e8c489"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
